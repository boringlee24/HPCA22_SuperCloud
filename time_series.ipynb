{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boxed-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from util import *\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = str(Path.home())\n",
    "DATA_DIR = f'{home}/Downloads/datacenter-challenge'\n",
    "user_df = pd.read_csv(f'{DATA_DIR}/scheduler_data.csv')\n",
    "user_df['run_time_from_user_df']=user_df['time_end']-user_df['time_start']\n",
    "\n",
    "start = time.time()\n",
    "gpu_ts_main = pd.read_csv(f\"{DATA_DIR}/nvidia_smi.csv\")\n",
    "end = time.time()\n",
    "print (f\"Sampling freq : {gpu_ts_main.iloc[1]['timestamp']-gpu_ts_main.iloc[0]['timestamp']}\")\n",
    "# gpu_ts_main.shape #(477606994, 12)\n",
    "gpu_ts_mini=gpu_ts_main[['Node', 'gpu_index','pcie_link_width_current','timestamp','id_job','utilization_memory_pct','utilization_gpu_pct','power_draw_W','memory_used_MiB']]\n",
    "start = time.time()\n",
    "gpu_ts_mini=gpu_ts_mini.set_index('id_job')\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "start = time.time()\n",
    "hash_ts_length=Counter(gpu_ts_mini.index)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print (len(hash_ts_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (user_df.shape[0]!=len(user_df['id_job'].unique())): #if there are duplicates\n",
    "    tmp_df=user_df[user_df['id_job'].duplicated(keep=False)][['id_job', 'derived_ec', 'state', 'derived_es']] #may need to rewrite for 2021-IAP. it doesnt have these columns\n",
    "    idx_to_drop=tmp_df.index[tmp_df['state']!=3]\n",
    "    user_df_no_dup=user_df.drop(idx_to_drop)\n",
    "    assert (user_df_no_dup.shape[0]==len(user_df_no_dup['id_job'].unique())) #all unique now\n",
    "else:\n",
    "    user_df_no_dup=user_df\n",
    "set_ts=set(gpu_ts_mini.index)\n",
    "\n",
    "start = time.time()\n",
    "jobs_to_drop=[job for job in hash_ts_length if hash_ts_length[job]<300] #drop small length jobs. sample freq seems 100ms\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print (len(jobs_to_drop))\n",
    "\n",
    "start = time.time()\n",
    "gpu_ts=gpu_ts_mini.drop(pd.Index(jobs_to_drop))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "hash_ts_length=Counter(gpu_ts.index)\n",
    "len(hash_ts_length)\n",
    "start = time.time()\n",
    "\n",
    "gpu_ts=gpu_ts.dropna() #removes NaN timestamps if any\n",
    "\n",
    "hash_ts_length=Counter(gpu_ts.index)\n",
    "len(hash_ts_length)\n",
    "\n",
    "gpu_ts_rst_idx=gpu_ts.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_jobs_to_rle_sm=dict()\n",
    "dict_jobs_to_rle_sm_active=dict()\n",
    "dict_jobs_to_rle_sm_all=dict()\n",
    "\n",
    "jobs_set=list(set(gpu_ts.index))\n",
    "\n",
    "start=time.time()\n",
    "for job in jobs_set:\n",
    "    series1=gpu_ts.loc[job]['utilization_gpu_pct']\n",
    "    dict_jobs_to_rle_sm_active[job]=find_active(series1)\n",
    "end=time.time()\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "for job in jobs_set:\n",
    "    series1=gpu_ts.loc[job]['utilization_gpu_pct']\n",
    "    dict_jobs_to_rle_sm_all[job]=find_all(series1)\n",
    "end=time.time()\n",
    "\n",
    "start=time.time()\n",
    "for job in jobs_set:\n",
    "    series1=gpu_ts.loc[job]['utilization_gpu_pct']\n",
    "    dict_jobs_to_rle_sm[job]=find(series1)\n",
    "end=time.time()\n",
    "end-start\n",
    "\n",
    "_object = dict_jobs_to_rle_sm_all\n",
    "_file = open('dict_jobs_to_rle_sm_all.obj', 'wb')\n",
    "pickle.dump(_object, _file) #https://www.thoughtco.com/using-pickle-to-save-objects-2813661\n",
    "_file.close()\n",
    "\n",
    "_object = dict_jobs_to_rle_sm_active\n",
    "_file = open('dict_active.obj', 'wb')\n",
    "pickle.dump(_object, _file) #https://www.thoughtco.com/using-pickle-to-save-objects-2813661\n",
    "_file.close()\n",
    "\n",
    "\n",
    "_object = dict_jobs_to_rle_sm\n",
    "_file = open('dict.obj', 'wb')\n",
    "pickle.dump(_object, _file) #https://www.thoughtco.com/using-pickle-to-save-objects-2813661\n",
    "_file.close()\n",
    "\n",
    "\n",
    "_file = open('dict_active.obj', 'rb')\n",
    "dict_jobs_to_rle_sm_active=pickle.load(_file) #https://www.thoughtco.com/using-pickle-to-save-objects-2813661\n",
    "_file.close()\n",
    "\n",
    "\n",
    "_file = open('dict.obj', 'rb')\n",
    "dict_jobs_to_rle_sm=pickle.load(_file) #https://www.thoughtco.com/using-pickle-to-save-objects-2813661\n",
    "_file.close()\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['Idle_steps']=pd.Series(dtype=object)\n",
    "df['Active_steps_list']=pd.Series(dtype=object)\n",
    "df['All_steps_list']=pd.Series(dtype=object)\n",
    "df['job_id']=(dict_jobs_to_rle_sm.keys())\n",
    "df=df.set_index('job_id')\n",
    "for job in df.index:\n",
    "    df.loc[job,'Idle_steps']=dict_jobs_to_rle_sm[job]\n",
    "    df.loc[job,'Active_steps_list']=dict_jobs_to_rle_sm_active[job]\n",
    "    df.loc[job,'All_steps_list']=dict_jobs_to_rle_sm_all[job]\n",
    "\n",
    "df_ck1=df\n",
    "\n",
    "df_ck1['t_list']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ck1['total_idle_steps']=df_ck1['Idle_steps'].apply(lambda x: sum(x))\n",
    "df_ck1['total_active_steps_sanity']=df_ck1['Active_steps_list'].apply(lambda x: sum(x))\n",
    "for job in df_ck1.index:\n",
    "    df_ck1.loc[job,'total_steps']=hash_ts_length[job]\n",
    "df_ck1['Idle_steps_rm1']=df_ck1['Idle_steps'].apply(lambda x: x[1:])\n",
    "df_ck1['count_intervals_rm1']=df_ck1['Idle_steps_rm1'].apply(lambda x: len(x))\n",
    "df_ck1['idle_time_pct']=df_ck1['total_idle_steps']/df_ck1['total_steps']*100\n",
    "\n",
    "df_ck1['Active_Steps']=df_ck1['total_steps']-df_ck1['total_idle_steps']\n",
    "df_ck1['Active_Steps_pct']=df_ck1['Active_Steps']/df_ck1['total_steps']*100\n",
    "\n",
    "df_ck1=df_ck1.sort_values('Active_Steps_pct')\n",
    "\n",
    "df1=df_ck1[['total_steps','Active_Steps_pct']]\n",
    "\n",
    "df1=df1.sort_values('Active_Steps_pct')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (3.1,2.6)\n",
    "fig, axs = plt.subplots()\n",
    "x=(np.arange(len(df1))/(len(df1)-1))*100\n",
    "axs.plot(df1['Active_Steps_pct'],x,label=\"% Active Time\",linestyle='--',linewidth=2,color='blue')\n",
    "axs.set_xlabel('% Active Time',fontsize=18)\n",
    "axs.set_ylabel('Empirical CDF \\n(% of Jobs)',fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,100)\n",
    "\n",
    "axs.set_xticks([0,25,50,75,100])\n",
    "axs.set_yticks([0,25,50,75,100])\n",
    "plt.grid(color='darkgrey', linestyle=':')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
